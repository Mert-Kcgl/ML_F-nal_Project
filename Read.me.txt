ML Assignment — Dropout vs Graduate Classification

This project is a binary classification task that aims to predict whether a student will graduate or drop out based on student data.

Target variable: Target
Dropout (0): Student who dropped out
Graduate (1): Student who graduated

The Enrolled class is excluded from the analysis.


PROJECT CONTENT

Data loading and missing value check

Binary classification decision (removal of the Enrolled class)

Feature engineering (3 new features)

Train/Test split + StandardScaler

Baseline model: DummyClassifier

Two real models: Gradient Boosting (with GridSearchCV) and Random Forest

Evaluation: Accuracy, Macro F1, Confusion Matrix, ROC Curve, PR Curve, Error Analysis, Feature Importance


SETUP

Required libraries:

pandas

numpy

scikit-learn

matplotlib

seaborn


DATASET

File name: data.csv

Data loading in code:
df = pd.read_csv('data.csv', sep=';')


METHOD OVERVIEW (WHAT THE CODE DOES)

Missing Value Check

The dataset is checked for missing values using:
df.isnull().sum().sum()

Since the output is 0, no missing value imputation is performed.

Binary Classification Decision

The Enrolled class is removed from the target variable:
df = df[df.Target != 'Enrolled']

This converts the problem into a binary classification task: Dropout vs Graduate.

Feature Engineering

To better represent students’ academic performance across semesters, three new features are created:

Approval_Rate_1st = (1st semester approved) / (1st semester enrolled)
Approval_Rate_2nd = (2nd semester approved) / (2nd semester enrolled)

When the enrolled value is 0, the ratio is set to 0 to avoid division by zero.

Grade_Change = (2nd semester grade) − (1st semester grade)

This step highlights the impact of academic performance on graduation or dropout decisions.

Target Encoding

The target variable is encoded using LabelEncoder:

Dropout -> 0
Graduate -> 1

The class distribution is printed using:
df['Target'].value_counts()

Correlation Analysis (Top 10 Features)

Instead of displaying the full correlation matrix, the top 10 features most correlated with the target variable are selected and visualized using a heatmap. This improves readability and interpretability.

Train/Test Split

The dataset is split into 80 percent training and 20 percent testing data.
To preserve class proportions, stratification is applied:

train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

Feature Scaling (StandardScaler)

All features are standardized. To prevent data leakage, the scaler is fitted only on the training set:

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

Additional Visualizations

Boxplot of Approval_Rate_2nd by class to demonstrate the effect of feature engineering

PCA visualization in two dimensions to provide intuition about class separability.


MODELS

Baseline Model: DummyClassifier

A baseline model using the most_frequent strategy is applied.
The model always predicts the majority class.

DummyClassifier(strategy="most_frequent")

Model 1: Gradient Boosting (with GridSearchCV)

The Gradient Boosting model is trained using hyperparameter tuning:

n_estimators: 100, 200
learning_rate: 0.1
max_depth: 3, 4

Model selection metric: Macro F1 (f1_macro)
Cross-validation: StratifiedKFold with 5 folds

Model 2: Random Forest

For comparison, a Random Forest model with 200 trees is trained:

RandomForestClassifier(n_estimators=200)


EVALUATION (METRICS AND PLOTS)

Since accuracy alone is not sufficient, multiple metrics and visualizations are used:

Accuracy

Macro F1

Classification report (precision, recall, F1-score)

Normalized confusion matrix

ROC Curve and AUC comparison between Random Forest and Gradient Boosting

Precision–Recall Curve and Average Precision (AP)

The Precision–Recall curve is particularly useful for evaluating the detection performance of the Dropout class.

Error analysis: misclassified samples are listed and examined using key features

Feature importance: the top 10 most important features from the Random Forest model are visualized using a bar chart


RUNNING THE PROJECT

Place the data.csv file in the project directory.

Run the code file or notebook.


NOTE

This project is designed to satisfy the following report requirements:

Baseline model using DummyClassifier

At least two real machine learning models

Hyperparameter tuning using GridSearchCV for at least one model

Stratified train/test split and StratifiedKFold

Evaluation metrics beyond accuracy (F1, ROC-AUC, PR-AUC / AP)

Confusion matrix and performance curves (ROC and PR)

Error analysis and meaningful visualizations such as boxplots, heatmaps, and feature importance charts